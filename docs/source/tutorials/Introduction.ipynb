{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "006a11b1",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0ad481",
   "metadata": {},
   "source": [
    "In this tutorial, we will show how to use PyTupli to set up an efficient pipeline for offline reinforcement learning (RL) for a custom environment. This includes\n",
    "- creating a benchmark and uploading it to an instance of a TupliStorage,\n",
    "- re-loading this benchmark from the storage, \n",
    "- recording RL tuples of (state, action, reward, done) for this benchmark and uploading them to the storage, \n",
    "- creating a dataset from the stored episodes, and\n",
    "- training an offline RL agent using d3rlpy.\n",
    "\n",
    "You can skip the last part, but if you want to try that, you have to install the d3rlpy library using `pip install d3rlpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a833b5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gymnasium import spaces\n",
    "from gymnasium.envs.classic_control import utils, MountainCarEnv\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "\n",
    "import d3rlpy\n",
    "from d3rlpy.algos import DiscreteCQLConfig\n",
    "from d3rlpy.dataset import MDPDataset\n",
    "\n",
    "from pytupli.benchmark import TupliEnvWrapper\n",
    "from pytupli.storage import TupliAPIClient, TupliStorage, FileStorage\n",
    "from gymnasium import Env\n",
    "from pytupli.schema import ArtifactMetadata, FilterEQ, EpisodeMetadataCallback\n",
    "from pytupli.dataset import TupliDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473615ad",
   "metadata": {},
   "source": [
    "PyTupli has two storage options: A local FileStorage and using MongoDB as a backend in the TupliAPIClient. You can run this notebook with both storage types by adjusting the flag below. If you want to use the TupliAPIClient, follow the instructions in the Readme to start the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4832d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "STORAGE_FLAG = 'api'  # \"api\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6df180",
   "metadata": {},
   "source": [
    "### Creating a Custom Environment\n",
    "We will use the MountainCar example from gymnasium with a small modification: The cart is slowed down by wind in the horizontal direction. We load the wind data from a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed4fbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMountainCarEnv(MountainCarEnv):\n",
    "    \"\"\"\n",
    "    ## Description\n",
    "\n",
    "    The Mountain Car MDP is a deterministic MDP that consists of a car placed stochastically\n",
    "    at the bottom of a sinusoidal valley, with the only possible actions being the accelerations\n",
    "    that can be applied to the car in either direction. The goal of the MDP is to strategically\n",
    "    accelerate the car to reach the goal state on top of the right hill. There are two versions\n",
    "    of the mountain car domain in gymnasium: one with discrete actions and one with continuous.\n",
    "    This version is the one with discrete actions.\n",
    "\n",
    "    This MDP first appeared in [Andrew Moore's PhD Thesis (1990)](https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-209.pdf)\n",
    "\n",
    "    ```\n",
    "    @TECHREPORT{Moore90efficientmemory-based,\n",
    "        author = {Andrew William Moore},\n",
    "        title = {Efficient Memory-based Learning for Robot Control},\n",
    "        institution = {University of Cambridge},\n",
    "        year = {1990}\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    ## Observation Space\n",
    "\n",
    "    The observation is a `ndarray` with shape `(2,)` where the elements correspond to the following:\n",
    "\n",
    "    | Num | Observation                          | Min   | Max  | Unit         |\n",
    "    |-----|--------------------------------------|-------|------|--------------|\n",
    "    | 0   | position of the car along the x-axis | -1.2  | 0.6  | position (m) |\n",
    "    | 1   | velocity of the car                  | -0.07 | 0.07 | velocity (v) |\n",
    "\n",
    "    ## Action Space\n",
    "\n",
    "    There are 3 discrete deterministic actions:\n",
    "\n",
    "    - 0: Accelerate to the left\n",
    "    - 1: Don't accelerate\n",
    "    - 2: Accelerate to the right\n",
    "\n",
    "    ## Transition Dynamics:\n",
    "\n",
    "    Given an action, the mountain car follows the following transition dynamics:\n",
    "\n",
    "    *velocity<sub>t+1</sub> = velocity<sub>t</sub> + (action - 1) * force - cos(3 * position<sub>t</sub>) * gravity*\n",
    "\n",
    "    *position<sub>t+1</sub> = position<sub>t</sub> + velocity<sub>t+1</sub>*\n",
    "\n",
    "    where force = 0.001 and gravity = 0.0025. The collisions at either end are inelastic with the velocity set to 0\n",
    "    upon collision with the wall. The position is clipped to the range `[-1.2, 0.6]` and\n",
    "    velocity is clipped to the range `[-0.07, 0.07]`.\n",
    "\n",
    "    ## Reward:\n",
    "\n",
    "    The goal is to reach the flag placed on top of the right hill as quickly as possible, as such the agent is\n",
    "    penalised with a reward of -1 for each timestep.\n",
    "\n",
    "    ## Starting State\n",
    "\n",
    "    The position of the car is assigned a uniform random value in *[-0.6 , -0.4]*.\n",
    "    The starting velocity of the car is always assigned to 0.\n",
    "\n",
    "    ## Episode End\n",
    "\n",
    "    The episode ends if either of the following happens:\n",
    "    1. Termination: The position of the car is greater than or equal to 0.5 (the goal position on top of the right hill)\n",
    "    2. Truncation: The length of the episode is 200.\n",
    "\n",
    "    ## Arguments\n",
    "\n",
    "    Mountain Car has two parameters for `gymnasium.make` with `render_mode` and `goal_velocity`.\n",
    "    On reset, the `options` parameter allows the user to change the bounds used to determine the new random state.\n",
    "\n",
    "    ```python\n",
    "    >>> import gymnasium as gym\n",
    "    >>> env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\", goal_velocity=0.1)  # default goal_velocity=0\n",
    "    >>> env\n",
    "    <TimeLimit<OrderEnforcing<PassiveEnvChecker<MountainCarEnv<MountainCar-v0>>>>>\n",
    "    >>> env.reset(seed=123, options={\"x_init\": np.pi/2, \"y_init\": 0.5})  # default x_init=np.pi, y_init=1.0\n",
    "    (array([-0.46352962,  0.        ], dtype=float32), {})\n",
    "\n",
    "    ```\n",
    "\n",
    "    ## Version History\n",
    "\n",
    "    * v0: Initial versions release\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\n",
    "        'render_modes': ['human', 'rgb_array'],\n",
    "        'render_fps': 30,\n",
    "    }\n",
    "\n",
    "    def __init__(self, data_path: str, render_mode: Optional[str] = None, goal_velocity=0):\n",
    "        self.min_position = -1.2\n",
    "        self.max_position = 0.6\n",
    "        self.max_speed = 0.07\n",
    "        self.goal_position = 0.5\n",
    "        self.goal_velocity = goal_velocity\n",
    "        self.current_step = 0\n",
    "        self.data = pd.read_csv(data_path, index_col=0, header=None) * 0.01\n",
    "\n",
    "        self.force = 0.001\n",
    "        self.gravity = 0.0025\n",
    "\n",
    "        self.low = np.array([self.min_position, -self.max_speed], dtype=np.float32)\n",
    "        self.high = np.array([self.max_position, self.max_speed], dtype=np.float32)\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.screen_width = 600\n",
    "        self.screen_height = 400\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        self.isopen = True\n",
    "\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.observation_space = spaces.Box(self.low, self.high, dtype=np.float32)\n",
    "\n",
    "    def step(self, action: int):\n",
    "        assert self.action_space.contains(action), f'{action!r} ({type(action)}) invalid'\n",
    "\n",
    "        position, velocity = self.state\n",
    "        velocity += (\n",
    "            (action - 1) * self.force\n",
    "            + math.cos(3 * position) * (-self.gravity)\n",
    "            - self.data.loc[self.current_step].to_numpy().flatten()[0] * math.cos(position)\n",
    "        )\n",
    "        velocity = np.clip(velocity, -self.max_speed, self.max_speed)\n",
    "        position += velocity\n",
    "        position = np.clip(position, self.min_position, self.max_position)\n",
    "        if position == self.min_position and velocity < 0:\n",
    "            velocity = 0\n",
    "\n",
    "        terminated = bool(position >= self.goal_position and velocity >= self.goal_velocity)\n",
    "        reward = -1.0\n",
    "        self.current_step += 1\n",
    "\n",
    "        self.state = (position, velocity)\n",
    "        if self.render_mode == 'human':\n",
    "            self.render()\n",
    "        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\n",
    "        return np.array(self.state, dtype=np.float32), reward, terminated, False, {}\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        *,\n",
    "        seed: Optional[int] = None,\n",
    "        options: Optional[dict] = None,\n",
    "    ):\n",
    "        super().reset(seed=seed)\n",
    "        # Note that if you use custom reset bounds, it may lead to out-of-bound\n",
    "        # state/observations.\n",
    "        self.current_step = 0\n",
    "        low, high = utils.maybe_parse_reset_bounds(options, -0.6, -0.4)\n",
    "        self.state = np.array([self.np_random.uniform(low=low, high=high), 0])\n",
    "\n",
    "        if self.render_mode == 'human':\n",
    "            self.render()\n",
    "        return np.array(self.state, dtype=np.float32), {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a91c03a",
   "metadata": {},
   "source": [
    "### Serialize Environment for Upload\n",
    "As a next step, we want to upload our environment to our storage using PyTupli. For this, we will detach the csv file from the environment, upload it seperately, and replace the data attribute in the environment with the id of the stored object. This allows us to re-use artifacts such as csv files in multiple benchmarks. For example, consider a case where you only want to change one parameter within the environment, e.g., the maximum speed. You would have to create a new benchmark, but could re-use the csv file! PyTupli automatically recognizes such duplicates. \n",
    "\n",
    "To separate the csv file, we have to subclass the TupliEnvWrapper class and overwrite the `_serialize()` and `_deserialize()` members. The TupliEnvWrapper is essentially a gymnasium wrapper that records RL tuples in the `step()` function, but it has a lot of extra functionalities for interacting with the storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616712b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTupliEnvWrapper(TupliEnvWrapper):\n",
    "    def _serialize(self, env) -> Env:\n",
    "        related_data_sources = []\n",
    "        ds = env.unwrapped.data\n",
    "        metadata = ArtifactMetadata(name='test')\n",
    "        data_kwargs = {'header': None}\n",
    "        try:\n",
    "            content = ds.to_csv(encoding='utf-8', **data_kwargs)\n",
    "            content = content.encode(encoding='utf-8')\n",
    "        except Exception as e:\n",
    "            raise ValueError(f'Failed to serialize data source: {e}')\n",
    "\n",
    "        ds_storage_metadata = self.storage.store_artifact(artifact=content, metadata=metadata)\n",
    "        related_data_sources.append(ds_storage_metadata.id)\n",
    "        setattr(env.unwrapped, 'data', ds_storage_metadata.id)\n",
    "        return env, related_data_sources\n",
    "\n",
    "    @classmethod\n",
    "    def _deserialize(cls, env: Env, storage: TupliStorage) -> Env:\n",
    "        data_kwargs = {'header': None, 'index_col': 0}\n",
    "        ds = storage.load_artifact(env.unwrapped.data)\n",
    "        ds = ds.decode('utf-8')\n",
    "        d = io.StringIO(ds)\n",
    "        df = pd.read_csv(d, **data_kwargs)\n",
    "\n",
    "        env.unwrapped.data = df\n",
    "        return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb34ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which storag to use\n",
    "if STORAGE_FLAG == 'api':\n",
    "    storage = TupliAPIClient()\n",
    "elif STORAGE_FLAG == 'file':\n",
    "    storage = FileStorage()\n",
    "else:\n",
    "    raise ValueError(f\"Unknown storage flag: {STORAGE_FLAG}. Has to be 'api' or 'file'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca3961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the environment\n",
    "max_eps_length = 999\n",
    "data_path = '/home/hannah/Documents/Code/pytupli/docs/source/tutorials/data/wind_data.csv'\n",
    "env = TimeLimit(\n",
    "    CustomMountainCarEnv(render_mode=None, data_path=data_path), max_episode_steps=max_eps_length\n",
    ")\n",
    "# Now we can create the benchmark\n",
    "tupli_env = MyTupliEnvWrapper(env, storage=storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10728029",
   "metadata": {},
   "source": [
    "### Uploading and Downloading Benchmarks\n",
    "We will now upload the benchmark and download it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39598da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tupli_env.store(name='mountain-car-v0', description='Mountain Car v0 benchmark')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3330620e",
   "metadata": {},
   "source": [
    "Let us list the uploaded benchmarks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48777c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%system\n",
    "!pytupli list_benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40330560",
   "metadata": {},
   "source": [
    "As a next step, we show how to download the benchmark. Note that this is only for demonstration purposes! When loading the benchmark, we can pass a callback that will later be used to add metadate to recorded episodes. We provide a simple example of such a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab4089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCallback(EpisodeMetadataCallback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # we will compute the cumulative reward for an episode\n",
    "        self.cum_reward = 0\n",
    "        # Furthermore, we want to store the fact that the episode was not an expert episode\n",
    "        self.is_expert = False\n",
    "    def reset(self):\n",
    "        # we will compute the cumulative reward for an episode\n",
    "        self.cum_reward = 0\n",
    "    def __call__(self, tuple):\n",
    "        self.cum_reward += tuple.reward\n",
    "        return {\"cum_eps_reward\": [self.cum_reward], \"is_expert\": self.is_expert}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00674eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_tupli_env = MyTupliEnvWrapper.load(storage=storage, benchmark_id=tupli_env.id, metadata_callback=MyCallback())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac1fdb6",
   "metadata": {},
   "source": [
    "### Recording Episodes for Offline RL Training\n",
    "The TupliEnvWrapper wrapper allows us to record all interactions with the custom environment to the storage in form of tuples (state, action, reward, terminal, timeout). This can then be used for training an offline RL agent for this environment using any offline RL library. For simplicity, we will use a random policy to generate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a77d166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility when generating episodes\n",
    "np.random.seed(42)\n",
    "obs, info = loaded_tupli_env.reset(seed=42)\n",
    "\n",
    "for step in range(2000):\n",
    "    action = np.int64(np.random.randint(low=0, high=3))\n",
    "    obs, reward, done, truncated, info = loaded_tupli_env.step(action)\n",
    "    if done or truncated:\n",
    "        print(f'Episode finished after {step + 1} timesteps')\n",
    "        obs, info = loaded_tupli_env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0c56b0",
   "metadata": {},
   "source": [
    "### Downloading Episodes for a Benchmark\n",
    "Next, let us download all episodes that have been recorded for our benchmark. For this, we create a TupliDataset using a filter with the id of the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b299922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "mdp_dataset = TupliDataset(storage=storage).with_benchmark_filter(\n",
    "    FilterEQ(key='id', value=loaded_tupli_env.id)\n",
    ")\n",
    "mdp_dataset.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad448a82",
   "metadata": {},
   "source": [
    "We can show the contents of the dataset using the `preview()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dcb731",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp_dataset.preview()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d013c395",
   "metadata": {},
   "source": [
    "### Training an Offline RL Agent\n",
    "Finally, we use d3rlpy to train an offline RL agent on this environment. Note that we do not train it to convergence, we only show how to get from a PyTupli dataset to actually doing offline RL! Our TupliDataset has a method for converting all episodes into numpy arrays for states, actions, rewards, terminals, and timeouts. This can be customized if other output formats are required. Using these arrays, we can then create an `MDPDataset`, which is the required input format for all d3rlpy algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae80fb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, act, rew, terminal, truncated = mdp_dataset.convert_to_numpy()\n",
    "# create d3rlpy dataset\n",
    "d3rlpy_dataset = MDPDataset(\n",
    "    observations=obs, actions=act, rewards=rew, terminals=terminal, timeouts=truncated\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfd15af",
   "metadata": {},
   "source": [
    "Finally, let us show that training an agent using conservative Q-learning (CQL) works with this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501b2fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm for offline training: CQL from d3rlpy\n",
    "d3rlpy.seed(1)  # for reproducibility\n",
    "algo = DiscreteCQLConfig(batch_size=64, alpha=2.0, target_update_interval=1000).create(device='cpu')\n",
    "# train\n",
    "algo.fit(dataset=d3rlpy_dataset, n_steps=10000, n_steps_per_epoch=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7a0673",
   "metadata": {},
   "source": [
    "Let us test the trained policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18441c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate rendering\n",
    "setattr(loaded_tupli_env.unwrapped, 'render_mode', 'human')\n",
    "# deactivate recording of episodes\n",
    "loaded_tupli_env.deactivate_recording()\n",
    "# run the environment\n",
    "np.random.seed(seed=42)\n",
    "obs, info = loaded_tupli_env.reset(seed=42)\n",
    "\n",
    "for step in range(800):\n",
    "    action = np.int64(algo.predict(np.expand_dims(obs, axis=0))[0])\n",
    "    obs, reward, done, truncated, info = loaded_tupli_env.step(action)\n",
    "    if done or truncated:\n",
    "        print(f'Episode finished after {step + 1} timesteps')\n",
    "        obs, info = loaded_tupli_env.reset()\n",
    "# deactivate rendering\n",
    "loaded_tupli_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09181ff2",
   "metadata": {},
   "source": [
    "The trained policy manages to reach the flag even though it has only learned from random actions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e18a2c8",
   "metadata": {},
   "source": [
    "### Deleting Benchmarks\n",
    "To clean up our storage, we now delete the benchmark and all related artifacts. Episodes will automatically be deleted, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24f577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_tupli_env.delete(delete_artifacts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9681fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytupli_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
